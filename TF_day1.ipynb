{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-day1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**tensorflow is a open source library helps to develop and train machine learning models**\n",
        "And with this notebook we are starting to improve our TF knowledge "
      ],
      "metadata": {
        "id": "fvK6vgePXzcK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Fe6KEXHcQloH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf #first thing we do is importing the tf library "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Z5QkEDXtxT",
        "outputId": "0febc58e-aa97-4ac2-8acc-e6c1486943bf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we are going to load a prebuild dataset which is MNIST dataset which\n",
        "#contains handwritten digits\n",
        "(X_train,y_train), (X_test, y_test)= tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "dKofP9z-bCly"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and now we're going to scale our data as floting numbers, because it's \n",
        "#better to work on data which is scaled between same range \n",
        "X_train, X_test= X_train/255.0 ,X_test/255.0 "
      ],
      "metadata": {
        "id": "_3KVy-qbhneX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYl6rTDUo1s7",
        "outputId": "ea7d9dc0-76c4-4786-fccc-f0a3d183040b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is ready and and we also ready for **build our model**. First thing we need to check is input layer has the correct number of input features and we can specify it in first layer with input_shape argument. \n",
        "In this example we are going to use the a fully connected network structure with four layers.\n",
        "\n",
        "Fully connected layers are defined using the Dense class. We can specify the number of neurons in the layer as the first argument, and specify the activation function using the activation argument. We will use the ReLU which is really comman activation function used."
      ],
      "metadata": {
        "id": "2usBd_wZkclC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Sequential([\n",
        "                           tf.keras.layers.Flatten(input_shape=(28,28)), \n",
        "                           tf.keras.layers.Dense(128,activation='relu'),\n",
        "                           tf.keras.layers.Dropout(0.2),\n",
        "                           tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "pSF98sM5ijmk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for each example model returns a vector of  logits scores which is ordinarily then passed to a normalization function\n",
        "#that converts these logits to probabilities for each class and it is also possible to use softmax function last layer in our model.\n",
        "predictions = model(X_train[:1]).numpy()\n",
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu2G4B8tqP0z",
        "outputId": "94805530-6a05-4590-adbc-579ccdb78a56"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07525925, 0.09303825, 0.10056511, 0.08755996, 0.12320044,\n",
              "        0.15429336, 0.0968235 , 0.09028259, 0.10967118, 0.06930628]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#and now we are going to define a loss function for training\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss(y_train[:1], predictions).numpy()  #initial model's loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5BNl8bS7IGr",
        "outputId": "92a00842-0fb8-4962-ba71-6748e90af9f3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.8688996"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, we need to configure and compile the model and set the optimizer class to adam, set loss to the loss function which we defined abowe, and specify a metric to be evaluated for the model by setting the metrics parameter to accuracy."
      ],
      "metadata": {
        "id": "KEIxH3di9FlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "8jBfT6j69lj5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time to train our model\n",
        "model.fit(X_train,y_train,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sIR6zNV8yjH",
        "outputId": "70df9aba-9f18-4464-e372-868790c41230"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2936 - accuracy: 0.9141\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1434 - accuracy: 0.9577\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1056 - accuracy: 0.9682\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0857 - accuracy: 0.9730\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0752 - accuracy: 0.9759\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0666 - accuracy: 0.9791\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0586 - accuracy: 0.9811\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0532 - accuracy: 0.9824\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0470 - accuracy: 0.9850\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0438 - accuracy: 0.9856\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f677fa2f490>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#after training we need to check our model performance with test data which did not \n",
        "#used for training data\n",
        "model.evaluate(X_test,y_test,verbose=2) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I7feV7V-cwQ",
        "outputId": "67f5b18d-67fe-4cef-f1ea-f598eef10f51"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 0s - loss: 0.0712 - accuracy: 0.9800 - 385ms/epoch - 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07121668756008148, 0.9800000190734863]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "when we look the training error and the generalization error/test error everything seems good and our model's classify the data almost correctly."
      ],
      "metadata": {
        "id": "4wDADXhq_bDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rJy1wn-ikZ9m"
      }
    }
  ]
}